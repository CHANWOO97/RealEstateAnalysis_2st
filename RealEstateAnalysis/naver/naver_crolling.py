import requests
from bs4 import BeautifulSoup
import pandas as pd
import time


def get_news_data(query, year, max_pages=3):
    """í•´ë‹¹ ì—°ë„ì˜ ì•„íŒŒíŠ¸ ë§¤ë§¤ ê´€ë ¨ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ìˆ˜ì§‘"""
    news_data = []
    base_url = "https://search.naver.com/search.naver"

    for page in range(1, max_pages + 1):
        params = {
            "where": "news",
            "query": f"{query} {year} ì•„íŒŒíŠ¸ ë§¤ë§¤",
            "sm": "tab_pge",
            "start": (page - 1) * 10 + 1
        }

        response = requests.get(base_url, params=params, headers={"User-Agent": "Mozilla/5.0"})
        soup = BeautifulSoup(response.text, "html.parser")
        articles = soup.select("ul.list_news li")

        for article in articles:
            title_tag = article.select_one("a.news_tit")
            if title_tag:
                title = title_tag.text  # ë‰´ìŠ¤ ì œëª©
                link = title_tag["href"]  # ë‰´ìŠ¤ ë§í¬

                # ë³¸ë¬¸ í¬ë¡¤ë§
                content = get_news_content(link)
                news_data.append([year, title, content, link])
        time.sleep(1)  # ìš”ì²­ ê°„ê²© ì¡°ì ˆ

    return news_data


def get_news_content(url):
    """ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ (ì¼ë¶€ ì‚¬ì´íŠ¸ëŠ” í¬ë¡¤ë§ ì œí•œ)"""
    try:
        response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
        soup = BeautifulSoup(response.text, "html.parser")
        content = soup.get_text(separator=" ", strip=True)
        return content[:500]  # ë³¸ë¬¸ ë‚´ìš© ì¼ë¶€ (500ì)
    except:
        return "ë³¸ë¬¸ ì—†ìŒ"


# ğŸ“Œ íŠ¹ì • ì—°ë„ ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘
years = range(2019, 2025)
all_news = []

for year in years:
    print(f"{year}ë…„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì¤‘...")
    all_news.extend(get_news_data("ì•„íŒŒíŠ¸ ë§¤ë§¤", year))

# ğŸ“Œ ë°ì´í„° ì €ì¥
df_news = pd.DataFrame(all_news, columns=["ë…„ë„", "ì œëª©", "ë³¸ë¬¸", "ë§í¬"])
df_news.to_csv("news_data.csv", index=False, encoding="utf-8-sig")

print("ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
